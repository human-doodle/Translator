{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNVA8t8bg3PnuY6h6oxh0d6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/human-doodle/Translator/blob/master/Translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KPZkXB3uWw4",
        "colab_type": "text"
      },
      "source": [
        "# Stage 1: Dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5SZvj1mtAeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsxWOyPvufuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4Y2RyoJuhcC",
        "colab_type": "text"
      },
      "source": [
        "# Stage 2: Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL56p9a7ulVu",
        "colab_type": "text"
      },
      "source": [
        "## Loading files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNazQxOjurkX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ed4cf9ec-9dc0-42ea-ca57-94c9947ced6c"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh4RBfkiwdqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/Study/project /Transformer/data/europarl-v7.fr-en.en\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    europarl_en = f.read()\n",
        "with open(\"/content/drive/My Drive/Study/project /Transformer/data/europarl-v7.fr-en.fr\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    europarl_fr = f.read()\n",
        "with open(\"/content/drive/My Drive/Study/project /Transformer/data/nonbreaking_prefix.en\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    non_breaking_prefix_en = f.read()\n",
        "with open(\"/content/drive/My Drive/Study/project /Transformer/data/nonbreaking_prefix.fr\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    non_breaking_prefix_fr = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJJCFv2Sw6v-",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJk2XsUw0i0_",
        "colab_type": "text"
      },
      "source": [
        "Getting the non_breaking_prefixes as a clean list of words with a point at the end so it is easier to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSP-5Ieiw-yU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
        "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S967amgx0q2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_en = europarl_en\n",
        "# Add $$$ after non ending sentence points\n",
        "for prefix in non_breaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
        "# Remove $$$ markers\n",
        "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
        "# Clear multiple spaces\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
        "corpus_en = corpus_en.split('\\n')\n",
        "\n",
        "corpus_fr = europarl_fr\n",
        "for prefix in non_breaking_prefix_fr:\n",
        "    corpus_fr = corpus_fr.replace(prefix, prefix + '$$$')\n",
        "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_fr)\n",
        "corpus_fr = re.sub(r\".\\$\\$\\$\", '', corpus_fr)\n",
        "corpus_fr = re.sub(r\"  +\", \" \", corpus_fr)\n",
        "corpus_fr = corpus_fr.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9hK658b0yEO",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IivCI_4y0ugM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13)\n",
        "tokenizer_fr = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_fr, target_vocab_size=2**13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISF9yjhM07QN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y27ikVzg097b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1]\n",
        "           for sentence in corpus_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEokfY4m1hsf",
        "colab_type": "text"
      },
      "source": [
        "## Remove too long sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwWl34a71nZW",
        "colab_type": "text"
      },
      "source": [
        "because we don't have required server that can handle it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgfUorVN1gs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYtHrd9i2P_b",
        "colab_type": "text"
      },
      "source": [
        "## Input output/creator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUIsyaPa2XUP",
        "colab_type": "text"
      },
      "source": [
        "As we train with batches, we need each input to have the same length. We pad with the appropriate token, and we will make sure this padding token doesn't interfere with our training later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYGKMJ382Zht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=MAX_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tct15EFa2czI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHd4A2Qe48EU",
        "colab_type": "text"
      },
      "source": [
        "# Stage 3: Model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlgGkWp549na",
        "colab_type": "text"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G9C3ucmJ86I",
        "colab_type": "text"
      },
      "source": [
        "Positional encoding formulae:\n",
        "\n",
        "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
        "\n",
        "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46w0uOpf5Dhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "    \n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4MrvpMC5ebV",
        "colab_type": "text"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s7Ewjbs5kMf",
        "colab_type": "text"
      },
      "source": [
        "###Attention computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VBuW6lESLDX",
        "colab_type": "text"
      },
      "source": [
        "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY_IXm695o7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    \n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "    \n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "    \n",
        "    return attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUD7zpAP50ry",
        "colab_type": "text"
      },
      "source": [
        "### Multi-head attention sublayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9bscHMS8Ix4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    \n",
        "    def __init__(self, nb_proj):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        \n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "        \n",
        "        self.query_lin = layers.Dense(units=self.d_model)\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "        self.final_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "        shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.nb_proj,\n",
        "                 self.d_proj)\n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "    \n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "        \n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "        \n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "        \n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "        \n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        concat_attention = tf.reshape(attention,\n",
        "                                      shape=(batch_size, -1, self.d_model))\n",
        "        \n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAicoMCl-g-Z",
        "colab_type": "text"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyLActVs-kji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGYFdJa9BK6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g90vBEF_BL9_",
        "colab_type": "text"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPsMqIZOBVxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        # Self multi head attention\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Multi head attention combined with encoder output\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Feed foward\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
        "                                    activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        attention = self.multi_head_attention_1(inputs,\n",
        "                                                inputs,\n",
        "                                                inputs,\n",
        "                                                mask_1)\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\n",
        "        \n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEC9ByQeBPLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for i in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2,\n",
        "                                         training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xcd5Z_KgD3MC",
        "colab_type": "text"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJPGUtLXD-Om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        \n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
        "    \n",
        "    def create_padding_mask(self, seq):\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        \n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kznqeS8LFwBx",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiOdqQ5qPs8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46xg4Wrg1Wgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Goque362343",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb_32PIU5Zkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./drive/My Drive/Study/project /Transformer/ckpt\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhFK5kUx602K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8dfc32c4-7731-4534-ed19-8af2010d2349"
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 6.2669 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 6.3060 Accuracy 0.0027\n",
            "Epoch 1 Batch 100 Loss 6.2094 Accuracy 0.0269\n",
            "Epoch 1 Batch 150 Loss 6.1467 Accuracy 0.0355\n",
            "Epoch 1 Batch 200 Loss 6.0575 Accuracy 0.0400\n",
            "Epoch 1 Batch 250 Loss 5.9461 Accuracy 0.0438\n",
            "Epoch 1 Batch 300 Loss 5.8300 Accuracy 0.0513\n",
            "Epoch 1 Batch 350 Loss 5.6966 Accuracy 0.0572\n",
            "Epoch 1 Batch 400 Loss 5.5682 Accuracy 0.0618\n",
            "Epoch 1 Batch 450 Loss 5.4453 Accuracy 0.0655\n",
            "Epoch 1 Batch 500 Loss 5.3345 Accuracy 0.0698\n",
            "Epoch 1 Batch 550 Loss 5.2303 Accuracy 0.0751\n",
            "Epoch 1 Batch 600 Loss 5.1311 Accuracy 0.0806\n",
            "Epoch 1 Batch 650 Loss 5.0388 Accuracy 0.0860\n",
            "Epoch 1 Batch 700 Loss 4.9519 Accuracy 0.0911\n",
            "Epoch 1 Batch 750 Loss 4.8666 Accuracy 0.0961\n",
            "Epoch 1 Batch 800 Loss 4.7863 Accuracy 0.1010\n",
            "Epoch 1 Batch 850 Loss 4.7085 Accuracy 0.1060\n",
            "Epoch 1 Batch 900 Loss 4.6357 Accuracy 0.1108\n",
            "Epoch 1 Batch 950 Loss 4.5662 Accuracy 0.1153\n",
            "Epoch 1 Batch 1000 Loss 4.5007 Accuracy 0.1197\n",
            "Epoch 1 Batch 1050 Loss 4.4397 Accuracy 0.1239\n",
            "Epoch 1 Batch 1100 Loss 4.3813 Accuracy 0.1277\n",
            "Epoch 1 Batch 1150 Loss 4.3283 Accuracy 0.1313\n",
            "Epoch 1 Batch 1200 Loss 4.2777 Accuracy 0.1348\n",
            "Epoch 1 Batch 1250 Loss 4.2283 Accuracy 0.1381\n",
            "Epoch 1 Batch 1300 Loss 4.1813 Accuracy 0.1413\n",
            "Epoch 1 Batch 1350 Loss 4.1369 Accuracy 0.1444\n",
            "Epoch 1 Batch 1400 Loss 4.0958 Accuracy 0.1476\n",
            "Epoch 1 Batch 1450 Loss 4.0551 Accuracy 0.1507\n",
            "Epoch 1 Batch 1500 Loss 4.0161 Accuracy 0.1537\n",
            "Epoch 1 Batch 1550 Loss 3.9774 Accuracy 0.1567\n",
            "Epoch 1 Batch 1600 Loss 3.9415 Accuracy 0.1597\n",
            "Epoch 1 Batch 1650 Loss 3.9063 Accuracy 0.1625\n",
            "Epoch 1 Batch 1700 Loss 3.8729 Accuracy 0.1654\n",
            "Epoch 1 Batch 1750 Loss 3.8406 Accuracy 0.1680\n",
            "Epoch 1 Batch 1800 Loss 3.8095 Accuracy 0.1706\n",
            "Epoch 1 Batch 1850 Loss 3.7794 Accuracy 0.1732\n",
            "Epoch 1 Batch 1900 Loss 3.7496 Accuracy 0.1757\n",
            "Epoch 1 Batch 1950 Loss 3.7210 Accuracy 0.1782\n",
            "Epoch 1 Batch 2000 Loss 3.6942 Accuracy 0.1805\n",
            "Epoch 1 Batch 2050 Loss 3.6676 Accuracy 0.1826\n",
            "Epoch 1 Batch 2100 Loss 3.6402 Accuracy 0.1846\n",
            "Epoch 1 Batch 2150 Loss 3.6140 Accuracy 0.1864\n",
            "Epoch 1 Batch 2200 Loss 3.5878 Accuracy 0.1883\n",
            "Epoch 1 Batch 2250 Loss 3.5621 Accuracy 0.1901\n",
            "Epoch 1 Batch 2300 Loss 3.5367 Accuracy 0.1918\n",
            "Epoch 1 Batch 2350 Loss 3.5119 Accuracy 0.1937\n",
            "Epoch 1 Batch 2400 Loss 3.4883 Accuracy 0.1954\n",
            "Epoch 1 Batch 2450 Loss 3.4656 Accuracy 0.1971\n",
            "Epoch 1 Batch 2500 Loss 3.4426 Accuracy 0.1989\n",
            "Epoch 1 Batch 2550 Loss 3.4205 Accuracy 0.2006\n",
            "Epoch 1 Batch 2600 Loss 3.3986 Accuracy 0.2023\n",
            "Epoch 1 Batch 2650 Loss 3.3776 Accuracy 0.2041\n",
            "Epoch 1 Batch 2700 Loss 3.3568 Accuracy 0.2058\n",
            "Epoch 1 Batch 2750 Loss 3.3364 Accuracy 0.2075\n",
            "Epoch 1 Batch 2800 Loss 3.3164 Accuracy 0.2091\n",
            "Epoch 1 Batch 2850 Loss 3.2975 Accuracy 0.2108\n",
            "Epoch 1 Batch 2900 Loss 3.2795 Accuracy 0.2124\n",
            "Epoch 1 Batch 2950 Loss 3.2608 Accuracy 0.2141\n",
            "Epoch 1 Batch 3000 Loss 3.2426 Accuracy 0.2157\n",
            "Epoch 1 Batch 3050 Loss 3.2244 Accuracy 0.2174\n",
            "Epoch 1 Batch 3100 Loss 3.2068 Accuracy 0.2191\n",
            "Epoch 1 Batch 3150 Loss 3.1893 Accuracy 0.2206\n",
            "Epoch 1 Batch 3200 Loss 3.1717 Accuracy 0.2221\n",
            "Epoch 1 Batch 3250 Loss 3.1544 Accuracy 0.2237\n",
            "Epoch 1 Batch 3300 Loss 3.1375 Accuracy 0.2252\n",
            "Epoch 1 Batch 3350 Loss 3.1208 Accuracy 0.2268\n",
            "Epoch 1 Batch 3400 Loss 3.1045 Accuracy 0.2284\n",
            "Epoch 1 Batch 3450 Loss 3.0887 Accuracy 0.2299\n",
            "Epoch 1 Batch 3500 Loss 3.0734 Accuracy 0.2315\n",
            "Epoch 1 Batch 3550 Loss 3.0584 Accuracy 0.2330\n",
            "Epoch 1 Batch 3600 Loss 3.0428 Accuracy 0.2346\n",
            "Epoch 1 Batch 3650 Loss 3.0275 Accuracy 0.2362\n",
            "Epoch 1 Batch 3700 Loss 3.0128 Accuracy 0.2378\n",
            "Epoch 1 Batch 3750 Loss 2.9982 Accuracy 0.2394\n",
            "Epoch 1 Batch 3800 Loss 2.9838 Accuracy 0.2410\n",
            "Epoch 1 Batch 3850 Loss 2.9696 Accuracy 0.2426\n",
            "Epoch 1 Batch 3900 Loss 2.9552 Accuracy 0.2442\n",
            "Epoch 1 Batch 3950 Loss 2.9411 Accuracy 0.2457\n",
            "Epoch 1 Batch 4000 Loss 2.9277 Accuracy 0.2472\n",
            "Epoch 1 Batch 4050 Loss 2.9141 Accuracy 0.2488\n",
            "Epoch 1 Batch 4100 Loss 2.9007 Accuracy 0.2502\n",
            "Epoch 1 Batch 4150 Loss 2.8882 Accuracy 0.2516\n",
            "Epoch 1 Batch 4200 Loss 2.8762 Accuracy 0.2529\n",
            "Epoch 1 Batch 4250 Loss 2.8645 Accuracy 0.2541\n",
            "Epoch 1 Batch 4300 Loss 2.8534 Accuracy 0.2554\n",
            "Epoch 1 Batch 4350 Loss 2.8427 Accuracy 0.2565\n",
            "Epoch 1 Batch 4400 Loss 2.8319 Accuracy 0.2576\n",
            "Epoch 1 Batch 4450 Loss 2.8212 Accuracy 0.2588\n",
            "Epoch 1 Batch 4500 Loss 2.8113 Accuracy 0.2599\n",
            "Epoch 1 Batch 4550 Loss 2.8011 Accuracy 0.2610\n",
            "Epoch 1 Batch 4600 Loss 2.7912 Accuracy 0.2621\n",
            "Epoch 1 Batch 4650 Loss 2.7818 Accuracy 0.2631\n",
            "Epoch 1 Batch 4700 Loss 2.7719 Accuracy 0.2641\n",
            "Epoch 1 Batch 4750 Loss 2.7625 Accuracy 0.2652\n",
            "Epoch 1 Batch 4800 Loss 2.7528 Accuracy 0.2663\n",
            "Epoch 1 Batch 4850 Loss 2.7435 Accuracy 0.2673\n",
            "Epoch 1 Batch 4900 Loss 2.7342 Accuracy 0.2683\n",
            "Epoch 1 Batch 4950 Loss 2.7253 Accuracy 0.2693\n",
            "Epoch 1 Batch 5000 Loss 2.7163 Accuracy 0.2703\n",
            "Epoch 1 Batch 5050 Loss 2.7073 Accuracy 0.2713\n",
            "Epoch 1 Batch 5100 Loss 2.6984 Accuracy 0.2722\n",
            "Epoch 1 Batch 5150 Loss 2.6897 Accuracy 0.2731\n",
            "Epoch 1 Batch 5200 Loss 2.6808 Accuracy 0.2740\n",
            "Epoch 1 Batch 5250 Loss 2.6721 Accuracy 0.2749\n",
            "Epoch 1 Batch 5300 Loss 2.6634 Accuracy 0.2757\n",
            "Epoch 1 Batch 5350 Loss 2.6548 Accuracy 0.2766\n",
            "Epoch 1 Batch 5400 Loss 2.6463 Accuracy 0.2774\n",
            "Epoch 1 Batch 5450 Loss 2.6378 Accuracy 0.2783\n",
            "Epoch 1 Batch 5500 Loss 2.6297 Accuracy 0.2791\n",
            "Epoch 1 Batch 5550 Loss 2.6213 Accuracy 0.2799\n",
            "Epoch 1 Batch 5600 Loss 2.6130 Accuracy 0.2807\n",
            "Epoch 1 Batch 5650 Loss 2.6054 Accuracy 0.2816\n",
            "Epoch 1 Batch 5700 Loss 2.5973 Accuracy 0.2824\n",
            "Saving checkpoint for epoch 1 at ./drive/My Drive/Study/project /Transformer/ckpt/ckpt-1\n",
            "Time taken for 1 epoch: 7050.326678991318 secs\n",
            "\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 1.7717 Accuracy 0.3857\n",
            "Epoch 2 Batch 50 Loss 1.7425 Accuracy 0.3820\n",
            "Epoch 2 Batch 100 Loss 1.7234 Accuracy 0.3838\n",
            "Epoch 2 Batch 150 Loss 1.7176 Accuracy 0.3837\n",
            "Epoch 2 Batch 200 Loss 1.7126 Accuracy 0.3838\n",
            "Epoch 2 Batch 250 Loss 1.7043 Accuracy 0.3845\n",
            "Epoch 2 Batch 300 Loss 1.6976 Accuracy 0.3844\n",
            "Epoch 2 Batch 350 Loss 1.6911 Accuracy 0.3846\n",
            "Epoch 2 Batch 400 Loss 1.6856 Accuracy 0.3844\n",
            "Epoch 2 Batch 450 Loss 1.6795 Accuracy 0.3842\n",
            "Epoch 2 Batch 500 Loss 1.6710 Accuracy 0.3846\n",
            "Epoch 2 Batch 550 Loss 1.6663 Accuracy 0.3854\n",
            "Epoch 2 Batch 600 Loss 1.6632 Accuracy 0.3860\n",
            "Epoch 2 Batch 650 Loss 1.6594 Accuracy 0.3864\n",
            "Epoch 2 Batch 700 Loss 1.6570 Accuracy 0.3873\n",
            "Epoch 2 Batch 750 Loss 1.6553 Accuracy 0.3879\n",
            "Epoch 2 Batch 800 Loss 1.6527 Accuracy 0.3887\n",
            "Epoch 2 Batch 850 Loss 1.6501 Accuracy 0.3890\n",
            "Epoch 2 Batch 900 Loss 1.6471 Accuracy 0.3892\n",
            "Epoch 2 Batch 950 Loss 1.6436 Accuracy 0.3894\n",
            "Epoch 2 Batch 1000 Loss 1.6378 Accuracy 0.3898\n",
            "Epoch 2 Batch 1050 Loss 1.6342 Accuracy 0.3904\n",
            "Epoch 2 Batch 1100 Loss 1.6313 Accuracy 0.3906\n",
            "Epoch 2 Batch 1150 Loss 1.6272 Accuracy 0.3909\n",
            "Epoch 2 Batch 1200 Loss 1.6248 Accuracy 0.3912\n",
            "Epoch 2 Batch 1250 Loss 1.6204 Accuracy 0.3917\n",
            "Epoch 2 Batch 1300 Loss 1.6171 Accuracy 0.3923\n",
            "Epoch 2 Batch 1350 Loss 1.6145 Accuracy 0.3931\n",
            "Epoch 2 Batch 1400 Loss 1.6109 Accuracy 0.3938\n",
            "Epoch 2 Batch 1450 Loss 1.6079 Accuracy 0.3946\n",
            "Epoch 2 Batch 1500 Loss 1.6044 Accuracy 0.3955\n",
            "Epoch 2 Batch 1550 Loss 1.6009 Accuracy 0.3966\n",
            "Epoch 2 Batch 1600 Loss 1.5967 Accuracy 0.3976\n",
            "Epoch 2 Batch 1650 Loss 1.5930 Accuracy 0.3985\n",
            "Epoch 2 Batch 1700 Loss 1.5887 Accuracy 0.3994\n",
            "Epoch 2 Batch 1750 Loss 1.5855 Accuracy 0.4003\n",
            "Epoch 2 Batch 1800 Loss 1.5815 Accuracy 0.4014\n",
            "Epoch 2 Batch 1850 Loss 1.5776 Accuracy 0.4025\n",
            "Epoch 2 Batch 1900 Loss 1.5739 Accuracy 0.4036\n",
            "Epoch 2 Batch 1950 Loss 1.5708 Accuracy 0.4044\n",
            "Epoch 2 Batch 2000 Loss 1.5680 Accuracy 0.4052\n",
            "Epoch 2 Batch 2050 Loss 1.5638 Accuracy 0.4059\n",
            "Epoch 2 Batch 2100 Loss 1.5602 Accuracy 0.4065\n",
            "Epoch 2 Batch 2150 Loss 1.5558 Accuracy 0.4070\n",
            "Epoch 2 Batch 2200 Loss 1.5500 Accuracy 0.4074\n",
            "Epoch 2 Batch 2250 Loss 1.5455 Accuracy 0.4079\n",
            "Epoch 2 Batch 2300 Loss 1.5409 Accuracy 0.4084\n",
            "Epoch 2 Batch 2350 Loss 1.5360 Accuracy 0.4089\n",
            "Epoch 2 Batch 2400 Loss 1.5310 Accuracy 0.4094\n",
            "Epoch 2 Batch 2450 Loss 1.5268 Accuracy 0.4100\n",
            "Epoch 2 Batch 2500 Loss 1.5217 Accuracy 0.4104\n",
            "Epoch 2 Batch 2550 Loss 1.5168 Accuracy 0.4110\n",
            "Epoch 2 Batch 2600 Loss 1.5117 Accuracy 0.4116\n",
            "Epoch 2 Batch 2650 Loss 1.5070 Accuracy 0.4122\n",
            "Epoch 2 Batch 2700 Loss 1.5026 Accuracy 0.4128\n",
            "Epoch 2 Batch 2750 Loss 1.4983 Accuracy 0.4133\n",
            "Epoch 2 Batch 2800 Loss 1.4945 Accuracy 0.4139\n",
            "Epoch 2 Batch 2850 Loss 1.4902 Accuracy 0.4144\n",
            "Epoch 2 Batch 2900 Loss 1.4865 Accuracy 0.4149\n",
            "Epoch 2 Batch 2950 Loss 1.4827 Accuracy 0.4155\n",
            "Epoch 2 Batch 3000 Loss 1.4789 Accuracy 0.4160\n",
            "Epoch 2 Batch 3050 Loss 1.4750 Accuracy 0.4166\n",
            "Epoch 2 Batch 3100 Loss 1.4713 Accuracy 0.4171\n",
            "Epoch 2 Batch 3150 Loss 1.4674 Accuracy 0.4177\n",
            "Epoch 2 Batch 3200 Loss 1.4636 Accuracy 0.4181\n",
            "Epoch 2 Batch 3250 Loss 1.4598 Accuracy 0.4187\n",
            "Epoch 2 Batch 3300 Loss 1.4558 Accuracy 0.4193\n",
            "Epoch 2 Batch 3350 Loss 1.4517 Accuracy 0.4197\n",
            "Epoch 2 Batch 3400 Loss 1.4481 Accuracy 0.4203\n",
            "Epoch 2 Batch 3450 Loss 1.4443 Accuracy 0.4209\n",
            "Epoch 2 Batch 3500 Loss 1.4404 Accuracy 0.4215\n",
            "Epoch 2 Batch 3550 Loss 1.4370 Accuracy 0.4220\n",
            "Epoch 2 Batch 3600 Loss 1.4336 Accuracy 0.4225\n",
            "Epoch 2 Batch 3650 Loss 1.4301 Accuracy 0.4231\n",
            "Epoch 2 Batch 3700 Loss 1.4269 Accuracy 0.4236\n",
            "Epoch 2 Batch 3750 Loss 1.4239 Accuracy 0.4242\n",
            "Epoch 2 Batch 3800 Loss 1.4209 Accuracy 0.4248\n",
            "Epoch 2 Batch 3850 Loss 1.4182 Accuracy 0.4254\n",
            "Epoch 2 Batch 3900 Loss 1.4153 Accuracy 0.4259\n",
            "Epoch 2 Batch 3950 Loss 1.4124 Accuracy 0.4264\n",
            "Epoch 2 Batch 4000 Loss 1.4093 Accuracy 0.4270\n",
            "Epoch 2 Batch 4050 Loss 1.4065 Accuracy 0.4275\n",
            "Epoch 2 Batch 4100 Loss 1.4037 Accuracy 0.4280\n",
            "Epoch 2 Batch 4150 Loss 1.4013 Accuracy 0.4283\n",
            "Epoch 2 Batch 4200 Loss 1.3997 Accuracy 0.4286\n",
            "Epoch 2 Batch 4250 Loss 1.3987 Accuracy 0.4288\n",
            "Epoch 2 Batch 4300 Loss 1.3982 Accuracy 0.4290\n",
            "Epoch 2 Batch 4350 Loss 1.3978 Accuracy 0.4291\n",
            "Epoch 2 Batch 4400 Loss 1.3976 Accuracy 0.4292\n",
            "Epoch 2 Batch 4450 Loss 1.3974 Accuracy 0.4292\n",
            "Epoch 2 Batch 4500 Loss 1.3970 Accuracy 0.4293\n",
            "Epoch 2 Batch 4550 Loss 1.3970 Accuracy 0.4293\n",
            "Epoch 2 Batch 4600 Loss 1.3969 Accuracy 0.4293\n",
            "Epoch 2 Batch 4650 Loss 1.3969 Accuracy 0.4294\n",
            "Epoch 2 Batch 4700 Loss 1.3970 Accuracy 0.4294\n",
            "Epoch 2 Batch 4750 Loss 1.3966 Accuracy 0.4294\n",
            "Epoch 2 Batch 4800 Loss 1.3965 Accuracy 0.4295\n",
            "Epoch 2 Batch 4850 Loss 1.3961 Accuracy 0.4295\n",
            "Epoch 2 Batch 4900 Loss 1.3960 Accuracy 0.4295\n",
            "Epoch 2 Batch 4950 Loss 1.3958 Accuracy 0.4296\n",
            "Epoch 2 Batch 5000 Loss 1.3960 Accuracy 0.4297\n",
            "Epoch 2 Batch 5050 Loss 1.3960 Accuracy 0.4296\n",
            "Epoch 2 Batch 5100 Loss 1.3959 Accuracy 0.4296\n",
            "Epoch 2 Batch 5150 Loss 1.3958 Accuracy 0.4295\n",
            "Epoch 2 Batch 5200 Loss 1.3955 Accuracy 0.4295\n",
            "Epoch 2 Batch 5250 Loss 1.3954 Accuracy 0.4295\n",
            "Epoch 2 Batch 5300 Loss 1.3949 Accuracy 0.4294\n",
            "Epoch 2 Batch 5350 Loss 1.3946 Accuracy 0.4293\n",
            "Epoch 2 Batch 5400 Loss 1.3944 Accuracy 0.4293\n",
            "Epoch 2 Batch 5450 Loss 1.3941 Accuracy 0.4293\n",
            "Epoch 2 Batch 5500 Loss 1.3937 Accuracy 0.4292\n",
            "Epoch 2 Batch 5550 Loss 1.3931 Accuracy 0.4291\n",
            "Epoch 2 Batch 5600 Loss 1.3929 Accuracy 0.4291\n",
            "Epoch 2 Batch 5650 Loss 1.3924 Accuracy 0.4290\n",
            "Epoch 2 Batch 5700 Loss 1.3920 Accuracy 0.4290\n",
            "Saving checkpoint for epoch 2 at ./drive/My Drive/Study/project /Transformer/ckpt/ckpt-2\n",
            "Time taken for 1 epoch: 7141.473264694214 secs\n",
            "\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 1.4590 Accuracy 0.4301\n",
            "Epoch 3 Batch 50 Loss 1.3679 Accuracy 0.4328\n",
            "Epoch 3 Batch 100 Loss 1.3567 Accuracy 0.4339\n",
            "Epoch 3 Batch 150 Loss 1.3502 Accuracy 0.4349\n",
            "Epoch 3 Batch 200 Loss 1.3482 Accuracy 0.4354\n",
            "Epoch 3 Batch 250 Loss 1.3484 Accuracy 0.4351\n",
            "Epoch 3 Batch 300 Loss 1.3406 Accuracy 0.4350\n",
            "Epoch 3 Batch 350 Loss 1.3412 Accuracy 0.4353\n",
            "Epoch 3 Batch 400 Loss 1.3376 Accuracy 0.4353\n",
            "Epoch 3 Batch 450 Loss 1.3356 Accuracy 0.4359\n",
            "Epoch 3 Batch 500 Loss 1.3312 Accuracy 0.4361\n",
            "Epoch 3 Batch 550 Loss 1.3276 Accuracy 0.4362\n",
            "Epoch 3 Batch 600 Loss 1.3264 Accuracy 0.4366\n",
            "Epoch 3 Batch 650 Loss 1.3269 Accuracy 0.4367\n",
            "Epoch 3 Batch 700 Loss 1.3246 Accuracy 0.4365\n",
            "Epoch 3 Batch 750 Loss 1.3243 Accuracy 0.4368\n",
            "Epoch 3 Batch 800 Loss 1.3228 Accuracy 0.4371\n",
            "Epoch 3 Batch 850 Loss 1.3208 Accuracy 0.4373\n",
            "Epoch 3 Batch 900 Loss 1.3207 Accuracy 0.4373\n",
            "Epoch 3 Batch 950 Loss 1.3188 Accuracy 0.4375\n",
            "Epoch 3 Batch 1000 Loss 1.3166 Accuracy 0.4375\n",
            "Epoch 3 Batch 1050 Loss 1.3147 Accuracy 0.4374\n",
            "Epoch 3 Batch 1100 Loss 1.3134 Accuracy 0.4378\n",
            "Epoch 3 Batch 1150 Loss 1.3114 Accuracy 0.4380\n",
            "Epoch 3 Batch 1200 Loss 1.3094 Accuracy 0.4382\n",
            "Epoch 3 Batch 1250 Loss 1.3078 Accuracy 0.4387\n",
            "Epoch 3 Batch 1300 Loss 1.3065 Accuracy 0.4391\n",
            "Epoch 3 Batch 1350 Loss 1.3043 Accuracy 0.4397\n",
            "Epoch 3 Batch 1400 Loss 1.3018 Accuracy 0.4403\n",
            "Epoch 3 Batch 1450 Loss 1.2984 Accuracy 0.4410\n",
            "Epoch 3 Batch 1500 Loss 1.2955 Accuracy 0.4418\n",
            "Epoch 3 Batch 1550 Loss 1.2924 Accuracy 0.4428\n",
            "Epoch 3 Batch 1600 Loss 1.2893 Accuracy 0.4437\n",
            "Epoch 3 Batch 1650 Loss 1.2863 Accuracy 0.4445\n",
            "Epoch 3 Batch 1700 Loss 1.2832 Accuracy 0.4454\n",
            "Epoch 3 Batch 1750 Loss 1.2811 Accuracy 0.4461\n",
            "Epoch 3 Batch 1800 Loss 1.2792 Accuracy 0.4470\n",
            "Epoch 3 Batch 1850 Loss 1.2764 Accuracy 0.4478\n",
            "Epoch 3 Batch 1900 Loss 1.2744 Accuracy 0.4487\n",
            "Epoch 3 Batch 1950 Loss 1.2718 Accuracy 0.4495\n",
            "Epoch 3 Batch 2000 Loss 1.2694 Accuracy 0.4502\n",
            "Epoch 3 Batch 2050 Loss 1.2667 Accuracy 0.4508\n",
            "Epoch 3 Batch 2100 Loss 1.2636 Accuracy 0.4514\n",
            "Epoch 3 Batch 2150 Loss 1.2603 Accuracy 0.4517\n",
            "Epoch 3 Batch 2200 Loss 1.2569 Accuracy 0.4519\n",
            "Epoch 3 Batch 2250 Loss 1.2533 Accuracy 0.4520\n",
            "Epoch 3 Batch 2300 Loss 1.2501 Accuracy 0.4522\n",
            "Epoch 3 Batch 2350 Loss 1.2464 Accuracy 0.4526\n",
            "Epoch 3 Batch 2400 Loss 1.2433 Accuracy 0.4529\n",
            "Epoch 3 Batch 2450 Loss 1.2406 Accuracy 0.4533\n",
            "Epoch 3 Batch 2500 Loss 1.2371 Accuracy 0.4537\n",
            "Epoch 3 Batch 2550 Loss 1.2331 Accuracy 0.4541\n",
            "Epoch 3 Batch 2600 Loss 1.2303 Accuracy 0.4546\n",
            "Epoch 3 Batch 2650 Loss 1.2269 Accuracy 0.4549\n",
            "Epoch 3 Batch 2700 Loss 1.2233 Accuracy 0.4553\n",
            "Epoch 3 Batch 2750 Loss 1.2203 Accuracy 0.4556\n",
            "Epoch 3 Batch 2800 Loss 1.2176 Accuracy 0.4560\n",
            "Epoch 3 Batch 2850 Loss 1.2144 Accuracy 0.4564\n",
            "Epoch 3 Batch 2900 Loss 1.2120 Accuracy 0.4568\n",
            "Epoch 3 Batch 2950 Loss 1.2096 Accuracy 0.4572\n",
            "Epoch 3 Batch 3000 Loss 1.2071 Accuracy 0.4575\n",
            "Epoch 3 Batch 3050 Loss 1.2047 Accuracy 0.4579\n",
            "Epoch 3 Batch 3100 Loss 1.2027 Accuracy 0.4582\n",
            "Epoch 3 Batch 3150 Loss 1.2000 Accuracy 0.4585\n",
            "Epoch 3 Batch 3200 Loss 1.1971 Accuracy 0.4588\n",
            "Epoch 3 Batch 3250 Loss 1.1947 Accuracy 0.4591\n",
            "Epoch 3 Batch 3300 Loss 1.1922 Accuracy 0.4594\n",
            "Epoch 3 Batch 3350 Loss 1.1895 Accuracy 0.4598\n",
            "Epoch 3 Batch 3400 Loss 1.1869 Accuracy 0.4602\n",
            "Epoch 3 Batch 3450 Loss 1.1846 Accuracy 0.4605\n",
            "Epoch 3 Batch 3500 Loss 1.1822 Accuracy 0.4610\n",
            "Epoch 3 Batch 3550 Loss 1.1800 Accuracy 0.4615\n",
            "Epoch 3 Batch 3600 Loss 1.1774 Accuracy 0.4619\n",
            "Epoch 3 Batch 3650 Loss 1.1751 Accuracy 0.4623\n",
            "Epoch 3 Batch 3700 Loss 1.1731 Accuracy 0.4628\n",
            "Epoch 3 Batch 3750 Loss 1.1709 Accuracy 0.4632\n",
            "Epoch 3 Batch 3800 Loss 1.1683 Accuracy 0.4636\n",
            "Epoch 3 Batch 3850 Loss 1.1665 Accuracy 0.4640\n",
            "Epoch 3 Batch 3900 Loss 1.1646 Accuracy 0.4644\n",
            "Epoch 3 Batch 3950 Loss 1.1624 Accuracy 0.4647\n",
            "Epoch 3 Batch 4000 Loss 1.1608 Accuracy 0.4651\n",
            "Epoch 3 Batch 4050 Loss 1.1593 Accuracy 0.4654\n",
            "Epoch 3 Batch 4100 Loss 1.1576 Accuracy 0.4657\n",
            "Epoch 3 Batch 4150 Loss 1.1566 Accuracy 0.4659\n",
            "Epoch 3 Batch 4200 Loss 1.1563 Accuracy 0.4660\n",
            "Epoch 3 Batch 4250 Loss 1.1563 Accuracy 0.4661\n",
            "Epoch 3 Batch 4300 Loss 1.1566 Accuracy 0.4661\n",
            "Epoch 3 Batch 4350 Loss 1.1572 Accuracy 0.4661\n",
            "Epoch 3 Batch 4400 Loss 1.1578 Accuracy 0.4660\n",
            "Epoch 3 Batch 4450 Loss 1.1586 Accuracy 0.4658\n",
            "Epoch 3 Batch 4500 Loss 1.1596 Accuracy 0.4657\n",
            "Epoch 3 Batch 4550 Loss 1.1601 Accuracy 0.4656\n",
            "Epoch 3 Batch 4600 Loss 1.1611 Accuracy 0.4654\n",
            "Epoch 3 Batch 4650 Loss 1.1622 Accuracy 0.4653\n",
            "Epoch 3 Batch 4700 Loss 1.1630 Accuracy 0.4652\n",
            "Epoch 3 Batch 4750 Loss 1.1639 Accuracy 0.4651\n",
            "Epoch 3 Batch 4800 Loss 1.1648 Accuracy 0.4650\n",
            "Epoch 3 Batch 4850 Loss 1.1657 Accuracy 0.4649\n",
            "Epoch 3 Batch 4900 Loss 1.1663 Accuracy 0.4648\n",
            "Epoch 3 Batch 4950 Loss 1.1670 Accuracy 0.4648\n",
            "Epoch 3 Batch 5000 Loss 1.1677 Accuracy 0.4646\n",
            "Epoch 3 Batch 5050 Loss 1.1685 Accuracy 0.4645\n",
            "Epoch 3 Batch 5100 Loss 1.1693 Accuracy 0.4644\n",
            "Epoch 3 Batch 5150 Loss 1.1702 Accuracy 0.4642\n",
            "Epoch 3 Batch 5200 Loss 1.1712 Accuracy 0.4640\n",
            "Epoch 3 Batch 5250 Loss 1.1715 Accuracy 0.4639\n",
            "Epoch 3 Batch 5300 Loss 1.1722 Accuracy 0.4637\n",
            "Epoch 3 Batch 5350 Loss 1.1730 Accuracy 0.4634\n",
            "Epoch 3 Batch 5400 Loss 1.1738 Accuracy 0.4632\n",
            "Epoch 3 Batch 5450 Loss 1.1744 Accuracy 0.4630\n",
            "Epoch 3 Batch 5500 Loss 1.1749 Accuracy 0.4629\n",
            "Epoch 3 Batch 5550 Loss 1.1755 Accuracy 0.4627\n",
            "Epoch 3 Batch 5600 Loss 1.1759 Accuracy 0.4624\n",
            "Epoch 3 Batch 5650 Loss 1.1765 Accuracy 0.4623\n",
            "Epoch 3 Batch 5700 Loss 1.1771 Accuracy 0.4621\n",
            "Saving checkpoint for epoch 3 at ./drive/My Drive/Study/project /Transformer/ckpt/ckpt-3\n",
            "Time taken for 1 epoch: 7106.876712799072 secs\n",
            "\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 1.1895 Accuracy 0.4589\n",
            "Epoch 4 Batch 50 Loss 1.2486 Accuracy 0.4528\n",
            "Epoch 4 Batch 100 Loss 1.2285 Accuracy 0.4515\n",
            "Epoch 4 Batch 150 Loss 1.2334 Accuracy 0.4540\n",
            "Epoch 4 Batch 200 Loss 1.2315 Accuracy 0.4542\n",
            "Epoch 4 Batch 250 Loss 1.2270 Accuracy 0.4540\n",
            "Epoch 4 Batch 300 Loss 1.2289 Accuracy 0.4542\n",
            "Epoch 4 Batch 350 Loss 1.2250 Accuracy 0.4542\n",
            "Epoch 4 Batch 400 Loss 1.2204 Accuracy 0.4541\n",
            "Epoch 4 Batch 450 Loss 1.2163 Accuracy 0.4542\n",
            "Epoch 4 Batch 500 Loss 1.2117 Accuracy 0.4546\n",
            "Epoch 4 Batch 550 Loss 1.2087 Accuracy 0.4548\n",
            "Epoch 4 Batch 600 Loss 1.2070 Accuracy 0.4549\n",
            "Epoch 4 Batch 650 Loss 1.2067 Accuracy 0.4551\n",
            "Epoch 4 Batch 700 Loss 1.2072 Accuracy 0.4554\n",
            "Epoch 4 Batch 750 Loss 1.2069 Accuracy 0.4557\n",
            "Epoch 4 Batch 800 Loss 1.2063 Accuracy 0.4558\n",
            "Epoch 4 Batch 850 Loss 1.2048 Accuracy 0.4558\n",
            "Epoch 4 Batch 900 Loss 1.2037 Accuracy 0.4559\n",
            "Epoch 4 Batch 950 Loss 1.2010 Accuracy 0.4558\n",
            "Epoch 4 Batch 1000 Loss 1.1975 Accuracy 0.4558\n",
            "Epoch 4 Batch 1050 Loss 1.1965 Accuracy 0.4559\n",
            "Epoch 4 Batch 1100 Loss 1.1959 Accuracy 0.4561\n",
            "Epoch 4 Batch 1150 Loss 1.1959 Accuracy 0.4562\n",
            "Epoch 4 Batch 1200 Loss 1.1955 Accuracy 0.4565\n",
            "Epoch 4 Batch 1250 Loss 1.1932 Accuracy 0.4569\n",
            "Epoch 4 Batch 1300 Loss 1.1911 Accuracy 0.4573\n",
            "Epoch 4 Batch 1350 Loss 1.1889 Accuracy 0.4577\n",
            "Epoch 4 Batch 1400 Loss 1.1868 Accuracy 0.4583\n",
            "Epoch 4 Batch 1450 Loss 1.1842 Accuracy 0.4591\n",
            "Epoch 4 Batch 1500 Loss 1.1817 Accuracy 0.4599\n",
            "Epoch 4 Batch 1550 Loss 1.1786 Accuracy 0.4608\n",
            "Epoch 4 Batch 1600 Loss 1.1764 Accuracy 0.4616\n",
            "Epoch 4 Batch 1650 Loss 1.1739 Accuracy 0.4623\n",
            "Epoch 4 Batch 1700 Loss 1.1713 Accuracy 0.4633\n",
            "Epoch 4 Batch 1750 Loss 1.1689 Accuracy 0.4641\n",
            "Epoch 4 Batch 1800 Loss 1.1667 Accuracy 0.4650\n",
            "Epoch 4 Batch 1850 Loss 1.1646 Accuracy 0.4657\n",
            "Epoch 4 Batch 1900 Loss 1.1624 Accuracy 0.4666\n",
            "Epoch 4 Batch 1950 Loss 1.1605 Accuracy 0.4674\n",
            "Epoch 4 Batch 2000 Loss 1.1582 Accuracy 0.4678\n",
            "Epoch 4 Batch 2050 Loss 1.1556 Accuracy 0.4681\n",
            "Epoch 4 Batch 2100 Loss 1.1530 Accuracy 0.4685\n",
            "Epoch 4 Batch 2150 Loss 1.1498 Accuracy 0.4687\n",
            "Epoch 4 Batch 2200 Loss 1.1467 Accuracy 0.4690\n",
            "Epoch 4 Batch 2250 Loss 1.1440 Accuracy 0.4693\n",
            "Epoch 4 Batch 2300 Loss 1.1412 Accuracy 0.4695\n",
            "Epoch 4 Batch 2350 Loss 1.1382 Accuracy 0.4698\n",
            "Epoch 4 Batch 2400 Loss 1.1347 Accuracy 0.4702\n",
            "Epoch 4 Batch 2450 Loss 1.1319 Accuracy 0.4705\n",
            "Epoch 4 Batch 2500 Loss 1.1287 Accuracy 0.4708\n",
            "Epoch 4 Batch 2550 Loss 1.1255 Accuracy 0.4711\n",
            "Epoch 4 Batch 2600 Loss 1.1220 Accuracy 0.4715\n",
            "Epoch 4 Batch 2650 Loss 1.1187 Accuracy 0.4719\n",
            "Epoch 4 Batch 2700 Loss 1.1159 Accuracy 0.4723\n",
            "Epoch 4 Batch 2750 Loss 1.1132 Accuracy 0.4726\n",
            "Epoch 4 Batch 2800 Loss 1.1112 Accuracy 0.4729\n",
            "Epoch 4 Batch 2850 Loss 1.1093 Accuracy 0.4732\n",
            "Epoch 4 Batch 2900 Loss 1.1071 Accuracy 0.4735\n",
            "Epoch 4 Batch 2950 Loss 1.1043 Accuracy 0.4739\n",
            "Epoch 4 Batch 3000 Loss 1.1024 Accuracy 0.4742\n",
            "Epoch 4 Batch 3050 Loss 1.0999 Accuracy 0.4745\n",
            "Epoch 4 Batch 3100 Loss 1.0978 Accuracy 0.4749\n",
            "Epoch 4 Batch 3150 Loss 1.0958 Accuracy 0.4752\n",
            "Epoch 4 Batch 3200 Loss 1.0938 Accuracy 0.4754\n",
            "Epoch 4 Batch 3250 Loss 1.0916 Accuracy 0.4757\n",
            "Epoch 4 Batch 3300 Loss 1.0893 Accuracy 0.4760\n",
            "Epoch 4 Batch 3350 Loss 1.0866 Accuracy 0.4764\n",
            "Epoch 4 Batch 3400 Loss 1.0840 Accuracy 0.4768\n",
            "Epoch 4 Batch 3450 Loss 1.0820 Accuracy 0.4771\n",
            "Epoch 4 Batch 3500 Loss 1.0800 Accuracy 0.4774\n",
            "Epoch 4 Batch 3550 Loss 1.0782 Accuracy 0.4777\n",
            "Epoch 4 Batch 3600 Loss 1.0761 Accuracy 0.4781\n",
            "Epoch 4 Batch 3650 Loss 1.0743 Accuracy 0.4784\n",
            "Epoch 4 Batch 3700 Loss 1.0724 Accuracy 0.4788\n",
            "Epoch 4 Batch 3750 Loss 1.0707 Accuracy 0.4791\n",
            "Epoch 4 Batch 3800 Loss 1.0690 Accuracy 0.4795\n",
            "Epoch 4 Batch 3850 Loss 1.0674 Accuracy 0.4799\n",
            "Epoch 4 Batch 3900 Loss 1.0661 Accuracy 0.4803\n",
            "Epoch 4 Batch 3950 Loss 1.0645 Accuracy 0.4806\n",
            "Epoch 4 Batch 4000 Loss 1.0627 Accuracy 0.4809\n",
            "Epoch 4 Batch 4050 Loss 1.0612 Accuracy 0.4812\n",
            "Epoch 4 Batch 4100 Loss 1.0604 Accuracy 0.4815\n",
            "Epoch 4 Batch 4150 Loss 1.0595 Accuracy 0.4816\n",
            "Epoch 4 Batch 4200 Loss 1.0592 Accuracy 0.4817\n",
            "Epoch 4 Batch 4250 Loss 1.0595 Accuracy 0.4817\n",
            "Epoch 4 Batch 4300 Loss 1.0599 Accuracy 0.4817\n",
            "Epoch 4 Batch 4350 Loss 1.0607 Accuracy 0.4816\n",
            "Epoch 4 Batch 4400 Loss 1.0616 Accuracy 0.4815\n",
            "Epoch 4 Batch 4450 Loss 1.0624 Accuracy 0.4814\n",
            "Epoch 4 Batch 4500 Loss 1.0635 Accuracy 0.4812\n",
            "Epoch 4 Batch 4550 Loss 1.0647 Accuracy 0.4811\n",
            "Epoch 4 Batch 4600 Loss 1.0659 Accuracy 0.4809\n",
            "Epoch 4 Batch 4650 Loss 1.0672 Accuracy 0.4808\n",
            "Epoch 4 Batch 4700 Loss 1.0686 Accuracy 0.4806\n",
            "Epoch 4 Batch 4750 Loss 1.0697 Accuracy 0.4804\n",
            "Epoch 4 Batch 4800 Loss 1.0707 Accuracy 0.4803\n",
            "Epoch 4 Batch 4850 Loss 1.0715 Accuracy 0.4802\n",
            "Epoch 4 Batch 4900 Loss 1.0728 Accuracy 0.4800\n",
            "Epoch 4 Batch 4950 Loss 1.0737 Accuracy 0.4799\n",
            "Epoch 4 Batch 5000 Loss 1.0748 Accuracy 0.4796\n",
            "Epoch 4 Batch 5050 Loss 1.0758 Accuracy 0.4795\n",
            "Epoch 4 Batch 5100 Loss 1.0770 Accuracy 0.4793\n",
            "Epoch 4 Batch 5150 Loss 1.0781 Accuracy 0.4791\n",
            "Epoch 4 Batch 5200 Loss 1.0790 Accuracy 0.4789\n",
            "Epoch 4 Batch 5250 Loss 1.0800 Accuracy 0.4787\n",
            "Epoch 4 Batch 5300 Loss 1.0810 Accuracy 0.4784\n",
            "Epoch 4 Batch 5350 Loss 1.0818 Accuracy 0.4781\n",
            "Epoch 4 Batch 5400 Loss 1.0825 Accuracy 0.4779\n",
            "Epoch 4 Batch 5450 Loss 1.0834 Accuracy 0.4776\n",
            "Epoch 4 Batch 5500 Loss 1.0841 Accuracy 0.4774\n",
            "Epoch 4 Batch 5550 Loss 1.0849 Accuracy 0.4772\n",
            "Epoch 4 Batch 5600 Loss 1.0858 Accuracy 0.4770\n",
            "Epoch 4 Batch 5650 Loss 1.0866 Accuracy 0.4768\n",
            "Epoch 4 Batch 5700 Loss 1.0875 Accuracy 0.4766\n",
            "Saving checkpoint for epoch 4 at ./drive/My Drive/Study/project /Transformer/ckpt/ckpt-4\n",
            "Time taken for 1 epoch: 7096.059152126312 secs\n",
            "\n",
            "Start of epoch 5\n",
            "Epoch 5 Batch 0 Loss 1.1243 Accuracy 0.4490\n",
            "Epoch 5 Batch 50 Loss 1.1711 Accuracy 0.4659\n",
            "Epoch 5 Batch 100 Loss 1.1650 Accuracy 0.4657\n",
            "Epoch 5 Batch 150 Loss 1.1608 Accuracy 0.4659\n",
            "Epoch 5 Batch 200 Loss 1.1586 Accuracy 0.4646\n",
            "Epoch 5 Batch 250 Loss 1.1600 Accuracy 0.4646\n",
            "Epoch 5 Batch 300 Loss 1.1603 Accuracy 0.4647\n",
            "Epoch 5 Batch 350 Loss 1.1595 Accuracy 0.4652\n",
            "Epoch 5 Batch 400 Loss 1.1585 Accuracy 0.4646\n",
            "Epoch 5 Batch 450 Loss 1.1553 Accuracy 0.4642\n",
            "Epoch 5 Batch 500 Loss 1.1548 Accuracy 0.4646\n",
            "Epoch 5 Batch 550 Loss 1.1541 Accuracy 0.4644\n",
            "Epoch 5 Batch 600 Loss 1.1521 Accuracy 0.4643\n",
            "Epoch 5 Batch 650 Loss 1.1525 Accuracy 0.4643\n",
            "Epoch 5 Batch 700 Loss 1.1495 Accuracy 0.4648\n",
            "Epoch 5 Batch 750 Loss 1.1492 Accuracy 0.4653\n",
            "Epoch 5 Batch 800 Loss 1.1474 Accuracy 0.4652\n",
            "Epoch 5 Batch 850 Loss 1.1454 Accuracy 0.4652\n",
            "Epoch 5 Batch 900 Loss 1.1437 Accuracy 0.4651\n",
            "Epoch 5 Batch 950 Loss 1.1417 Accuracy 0.4654\n",
            "Epoch 5 Batch 1000 Loss 1.1404 Accuracy 0.4653\n",
            "Epoch 5 Batch 1050 Loss 1.1388 Accuracy 0.4658\n",
            "Epoch 5 Batch 1100 Loss 1.1370 Accuracy 0.4663\n",
            "Epoch 5 Batch 1150 Loss 1.1365 Accuracy 0.4665\n",
            "Epoch 5 Batch 1200 Loss 1.1352 Accuracy 0.4666\n",
            "Epoch 5 Batch 1250 Loss 1.1338 Accuracy 0.4668\n",
            "Epoch 5 Batch 1300 Loss 1.1307 Accuracy 0.4675\n",
            "Epoch 5 Batch 1350 Loss 1.1287 Accuracy 0.4679\n",
            "Epoch 5 Batch 1400 Loss 1.1269 Accuracy 0.4685\n",
            "Epoch 5 Batch 1450 Loss 1.1245 Accuracy 0.4692\n",
            "Epoch 5 Batch 1500 Loss 1.1215 Accuracy 0.4699\n",
            "Epoch 5 Batch 1550 Loss 1.1188 Accuracy 0.4708\n",
            "Epoch 5 Batch 1600 Loss 1.1161 Accuracy 0.4717\n",
            "Epoch 5 Batch 1650 Loss 1.1133 Accuracy 0.4726\n",
            "Epoch 5 Batch 1700 Loss 1.1105 Accuracy 0.4735\n",
            "Epoch 5 Batch 1750 Loss 1.1081 Accuracy 0.4742\n",
            "Epoch 5 Batch 1800 Loss 1.1058 Accuracy 0.4750\n",
            "Epoch 5 Batch 1850 Loss 1.1038 Accuracy 0.4759\n",
            "Epoch 5 Batch 1900 Loss 1.1015 Accuracy 0.4767\n",
            "Epoch 5 Batch 1950 Loss 1.0995 Accuracy 0.4775\n",
            "Epoch 5 Batch 2000 Loss 1.0976 Accuracy 0.4781\n",
            "Epoch 5 Batch 2050 Loss 1.0947 Accuracy 0.4786\n",
            "Epoch 5 Batch 2100 Loss 1.0921 Accuracy 0.4790\n",
            "Epoch 5 Batch 2150 Loss 1.0893 Accuracy 0.4791\n",
            "Epoch 5 Batch 2200 Loss 1.0858 Accuracy 0.4792\n",
            "Epoch 5 Batch 2250 Loss 1.0825 Accuracy 0.4793\n",
            "Epoch 5 Batch 2300 Loss 1.0795 Accuracy 0.4796\n",
            "Epoch 5 Batch 2350 Loss 1.0766 Accuracy 0.4798\n",
            "Epoch 5 Batch 2400 Loss 1.0738 Accuracy 0.4800\n",
            "Epoch 5 Batch 2450 Loss 1.0709 Accuracy 0.4803\n",
            "Epoch 5 Batch 2500 Loss 1.0679 Accuracy 0.4806\n",
            "Epoch 5 Batch 2550 Loss 1.0656 Accuracy 0.4808\n",
            "Epoch 5 Batch 2600 Loss 1.0627 Accuracy 0.4813\n",
            "Epoch 5 Batch 2650 Loss 1.0599 Accuracy 0.4817\n",
            "Epoch 5 Batch 2700 Loss 1.0570 Accuracy 0.4821\n",
            "Epoch 5 Batch 2750 Loss 1.0542 Accuracy 0.4824\n",
            "Epoch 5 Batch 2800 Loss 1.0520 Accuracy 0.4828\n",
            "Epoch 5 Batch 2850 Loss 1.0499 Accuracy 0.4832\n",
            "Epoch 5 Batch 2900 Loss 1.0476 Accuracy 0.4834\n",
            "Epoch 5 Batch 2950 Loss 1.0450 Accuracy 0.4837\n",
            "Epoch 5 Batch 3000 Loss 1.0427 Accuracy 0.4840\n",
            "Epoch 5 Batch 3050 Loss 1.0408 Accuracy 0.4843\n",
            "Epoch 5 Batch 3100 Loss 1.0392 Accuracy 0.4846\n",
            "Epoch 5 Batch 3150 Loss 1.0371 Accuracy 0.4848\n",
            "Epoch 5 Batch 3200 Loss 1.0350 Accuracy 0.4850\n",
            "Epoch 5 Batch 3250 Loss 1.0325 Accuracy 0.4852\n",
            "Epoch 5 Batch 3300 Loss 1.0303 Accuracy 0.4854\n",
            "Epoch 5 Batch 3350 Loss 1.0279 Accuracy 0.4858\n",
            "Epoch 5 Batch 3400 Loss 1.0259 Accuracy 0.4861\n",
            "Epoch 5 Batch 3450 Loss 1.0241 Accuracy 0.4864\n",
            "Epoch 5 Batch 3500 Loss 1.0222 Accuracy 0.4868\n",
            "Epoch 5 Batch 3550 Loss 1.0204 Accuracy 0.4871\n",
            "Epoch 5 Batch 3600 Loss 1.0183 Accuracy 0.4875\n",
            "Epoch 5 Batch 3650 Loss 1.0165 Accuracy 0.4878\n",
            "Epoch 5 Batch 3700 Loss 1.0147 Accuracy 0.4881\n",
            "Epoch 5 Batch 3750 Loss 1.0130 Accuracy 0.4885\n",
            "Epoch 5 Batch 3800 Loss 1.0112 Accuracy 0.4888\n",
            "Epoch 5 Batch 3850 Loss 1.0097 Accuracy 0.4892\n",
            "Epoch 5 Batch 3900 Loss 1.0084 Accuracy 0.4896\n",
            "Epoch 5 Batch 3950 Loss 1.0067 Accuracy 0.4899\n",
            "Epoch 5 Batch 4000 Loss 1.0052 Accuracy 0.4903\n",
            "Epoch 5 Batch 4050 Loss 1.0038 Accuracy 0.4907\n",
            "Epoch 5 Batch 4100 Loss 1.0028 Accuracy 0.4909\n",
            "Epoch 5 Batch 4150 Loss 1.0021 Accuracy 0.4911\n",
            "Epoch 5 Batch 4200 Loss 1.0022 Accuracy 0.4911\n",
            "Epoch 5 Batch 4250 Loss 1.0030 Accuracy 0.4911\n",
            "Epoch 5 Batch 4300 Loss 1.0036 Accuracy 0.4910\n",
            "Epoch 5 Batch 4350 Loss 1.0044 Accuracy 0.4909\n",
            "Epoch 5 Batch 4400 Loss 1.0055 Accuracy 0.4908\n",
            "Epoch 5 Batch 4450 Loss 1.0067 Accuracy 0.4906\n",
            "Epoch 5 Batch 4500 Loss 1.0079 Accuracy 0.4904\n",
            "Epoch 5 Batch 4550 Loss 1.0090 Accuracy 0.4902\n",
            "Epoch 5 Batch 4600 Loss 1.0102 Accuracy 0.4900\n",
            "Epoch 5 Batch 4650 Loss 1.0113 Accuracy 0.4899\n",
            "Epoch 5 Batch 4700 Loss 1.0125 Accuracy 0.4897\n",
            "Epoch 5 Batch 4750 Loss 1.0138 Accuracy 0.4895\n",
            "Epoch 5 Batch 4800 Loss 1.0151 Accuracy 0.4894\n",
            "Epoch 5 Batch 4850 Loss 1.0161 Accuracy 0.4892\n",
            "Epoch 5 Batch 4900 Loss 1.0176 Accuracy 0.4891\n",
            "Epoch 5 Batch 4950 Loss 1.0185 Accuracy 0.4889\n",
            "Epoch 5 Batch 5000 Loss 1.0196 Accuracy 0.4888\n",
            "Epoch 5 Batch 5050 Loss 1.0209 Accuracy 0.4886\n",
            "Epoch 5 Batch 5100 Loss 1.0220 Accuracy 0.4883\n",
            "Epoch 5 Batch 5150 Loss 1.0231 Accuracy 0.4881\n",
            "Epoch 5 Batch 5200 Loss 1.0244 Accuracy 0.4878\n",
            "Epoch 5 Batch 5250 Loss 1.0256 Accuracy 0.4876\n",
            "Epoch 5 Batch 5300 Loss 1.0264 Accuracy 0.4873\n",
            "Epoch 5 Batch 5350 Loss 1.0276 Accuracy 0.4870\n",
            "Epoch 5 Batch 5400 Loss 1.0285 Accuracy 0.4867\n",
            "Epoch 5 Batch 5450 Loss 1.0299 Accuracy 0.4864\n",
            "Epoch 5 Batch 5500 Loss 1.0306 Accuracy 0.4862\n",
            "Epoch 5 Batch 5550 Loss 1.0314 Accuracy 0.4860\n",
            "Epoch 5 Batch 5600 Loss 1.0321 Accuracy 0.4858\n",
            "Epoch 5 Batch 5650 Loss 1.0331 Accuracy 0.4855\n",
            "Epoch 5 Batch 5700 Loss 1.0338 Accuracy 0.4853\n",
            "Saving checkpoint for epoch 5 at ./drive/My Drive/Study/project /Transformer/ckpt/ckpt-5\n",
            "Time taken for 1 epoch: 7208.330493211746 secs\n",
            "\n",
            "Start of epoch 6\n",
            "Epoch 6 Batch 0 Loss 1.1183 Accuracy 0.4992\n",
            "Epoch 6 Batch 50 Loss 1.1317 Accuracy 0.4706\n",
            "Epoch 6 Batch 100 Loss 1.1342 Accuracy 0.4721\n",
            "Epoch 6 Batch 150 Loss 1.1325 Accuracy 0.4717\n",
            "Epoch 6 Batch 200 Loss 1.1208 Accuracy 0.4713\n",
            "Epoch 6 Batch 250 Loss 1.1187 Accuracy 0.4719\n",
            "Epoch 6 Batch 300 Loss 1.1229 Accuracy 0.4716\n",
            "Epoch 6 Batch 350 Loss 1.1187 Accuracy 0.4714\n",
            "Epoch 6 Batch 400 Loss 1.1155 Accuracy 0.4711\n",
            "Epoch 6 Batch 450 Loss 1.1109 Accuracy 0.4716\n",
            "Epoch 6 Batch 500 Loss 1.1086 Accuracy 0.4709\n",
            "Epoch 6 Batch 550 Loss 1.1071 Accuracy 0.4714\n",
            "Epoch 6 Batch 600 Loss 1.1067 Accuracy 0.4714\n",
            "Epoch 6 Batch 650 Loss 1.1066 Accuracy 0.4714\n",
            "Epoch 6 Batch 700 Loss 1.1057 Accuracy 0.4713\n",
            "Epoch 6 Batch 750 Loss 1.1057 Accuracy 0.4719\n",
            "Epoch 6 Batch 800 Loss 1.1048 Accuracy 0.4723\n",
            "Epoch 6 Batch 850 Loss 1.1030 Accuracy 0.4723\n",
            "Epoch 6 Batch 900 Loss 1.1014 Accuracy 0.4724\n",
            "Epoch 6 Batch 950 Loss 1.0998 Accuracy 0.4725\n",
            "Epoch 6 Batch 1000 Loss 1.0963 Accuracy 0.4728\n",
            "Epoch 6 Batch 1050 Loss 1.0962 Accuracy 0.4726\n",
            "Epoch 6 Batch 1100 Loss 1.0938 Accuracy 0.4728\n",
            "Epoch 6 Batch 1150 Loss 1.0933 Accuracy 0.4729\n",
            "Epoch 6 Batch 1200 Loss 1.0914 Accuracy 0.4733\n",
            "Epoch 6 Batch 1250 Loss 1.0901 Accuracy 0.4735\n",
            "Epoch 6 Batch 1300 Loss 1.0877 Accuracy 0.4741\n",
            "Epoch 6 Batch 1350 Loss 1.0858 Accuracy 0.4746\n",
            "Epoch 6 Batch 1400 Loss 1.0833 Accuracy 0.4754\n",
            "Epoch 6 Batch 1450 Loss 1.0812 Accuracy 0.4761\n",
            "Epoch 6 Batch 1500 Loss 1.0788 Accuracy 0.4768\n",
            "Epoch 6 Batch 1550 Loss 1.0759 Accuracy 0.4777\n",
            "Epoch 6 Batch 1600 Loss 1.0739 Accuracy 0.4784\n",
            "Epoch 6 Batch 1650 Loss 1.0712 Accuracy 0.4794\n",
            "Epoch 6 Batch 1700 Loss 1.0691 Accuracy 0.4803\n",
            "Epoch 6 Batch 1750 Loss 1.0666 Accuracy 0.4810\n",
            "Epoch 6 Batch 1800 Loss 1.0643 Accuracy 0.4819\n",
            "Epoch 6 Batch 1850 Loss 1.0623 Accuracy 0.4826\n",
            "Epoch 6 Batch 1900 Loss 1.0600 Accuracy 0.4833\n",
            "Epoch 6 Batch 1950 Loss 1.0579 Accuracy 0.4840\n",
            "Epoch 6 Batch 2000 Loss 1.0555 Accuracy 0.4847\n",
            "Epoch 6 Batch 2050 Loss 1.0532 Accuracy 0.4852\n",
            "Epoch 6 Batch 2100 Loss 1.0509 Accuracy 0.4854\n",
            "Epoch 6 Batch 2150 Loss 1.0483 Accuracy 0.4858\n",
            "Epoch 6 Batch 2200 Loss 1.0452 Accuracy 0.4860\n",
            "Epoch 6 Batch 2250 Loss 1.0419 Accuracy 0.4862\n",
            "Epoch 6 Batch 2300 Loss 1.0389 Accuracy 0.4865\n",
            "Epoch 6 Batch 2350 Loss 1.0355 Accuracy 0.4868\n",
            "Epoch 6 Batch 2400 Loss 1.0325 Accuracy 0.4871\n",
            "Epoch 6 Batch 2450 Loss 1.0296 Accuracy 0.4874\n",
            "Epoch 6 Batch 2500 Loss 1.0271 Accuracy 0.4877\n",
            "Epoch 6 Batch 2550 Loss 1.0240 Accuracy 0.4881\n",
            "Epoch 6 Batch 2600 Loss 1.0213 Accuracy 0.4884\n",
            "Epoch 6 Batch 2650 Loss 1.0185 Accuracy 0.4888\n",
            "Epoch 6 Batch 2700 Loss 1.0158 Accuracy 0.4890\n",
            "Epoch 6 Batch 2750 Loss 1.0130 Accuracy 0.4891\n",
            "Epoch 6 Batch 2800 Loss 1.0109 Accuracy 0.4894\n",
            "Epoch 6 Batch 2850 Loss 1.0091 Accuracy 0.4897\n",
            "Epoch 6 Batch 2900 Loss 1.0075 Accuracy 0.4900\n",
            "Epoch 6 Batch 2950 Loss 1.0057 Accuracy 0.4903\n",
            "Epoch 6 Batch 3000 Loss 1.0034 Accuracy 0.4905\n",
            "Epoch 6 Batch 3050 Loss 1.0017 Accuracy 0.4908\n",
            "Epoch 6 Batch 3100 Loss 0.9999 Accuracy 0.4911\n",
            "Epoch 6 Batch 3150 Loss 0.9978 Accuracy 0.4913\n",
            "Epoch 6 Batch 3200 Loss 0.9956 Accuracy 0.4915\n",
            "Epoch 6 Batch 3250 Loss 0.9939 Accuracy 0.4918\n",
            "Epoch 6 Batch 3300 Loss 0.9920 Accuracy 0.4920\n",
            "Epoch 6 Batch 3350 Loss 0.9901 Accuracy 0.4924\n",
            "Epoch 6 Batch 3400 Loss 0.9876 Accuracy 0.4926\n",
            "Epoch 6 Batch 3450 Loss 0.9860 Accuracy 0.4928\n",
            "Epoch 6 Batch 3500 Loss 0.9842 Accuracy 0.4932\n",
            "Epoch 6 Batch 3550 Loss 0.9825 Accuracy 0.4935\n",
            "Epoch 6 Batch 3600 Loss 0.9805 Accuracy 0.4939\n",
            "Epoch 6 Batch 3650 Loss 0.9786 Accuracy 0.4942\n",
            "Epoch 6 Batch 3700 Loss 0.9767 Accuracy 0.4946\n",
            "Epoch 6 Batch 3750 Loss 0.9750 Accuracy 0.4949\n",
            "Epoch 6 Batch 3800 Loss 0.9736 Accuracy 0.4952\n",
            "Epoch 6 Batch 3850 Loss 0.9719 Accuracy 0.4955\n",
            "Epoch 6 Batch 3900 Loss 0.9704 Accuracy 0.4958\n",
            "Epoch 6 Batch 3950 Loss 0.9693 Accuracy 0.4962\n",
            "Epoch 6 Batch 4000 Loss 0.9679 Accuracy 0.4966\n",
            "Epoch 6 Batch 4050 Loss 0.9667 Accuracy 0.4969\n",
            "Epoch 6 Batch 4100 Loss 0.9656 Accuracy 0.4972\n",
            "Epoch 6 Batch 4150 Loss 0.9651 Accuracy 0.4973\n",
            "Epoch 6 Batch 4200 Loss 0.9649 Accuracy 0.4974\n",
            "Epoch 6 Batch 4250 Loss 0.9653 Accuracy 0.4973\n",
            "Epoch 6 Batch 4300 Loss 0.9658 Accuracy 0.4972\n",
            "Epoch 6 Batch 4350 Loss 0.9668 Accuracy 0.4971\n",
            "Epoch 6 Batch 4400 Loss 0.9679 Accuracy 0.4970\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1_LbxZbGWd0",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNHwJJrz3lPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\n",
        "    \n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        \n",
        "        prediction = predictions[:, -1:, :]\n",
        "        \n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        \n",
        "        if predicted_id == VOCAB_SIZE_FR-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6VeFKrE6Kdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    \n",
        "    predicted_sentence = tokenizer_fr.decode(\n",
        "        [i for i in output if i < VOCAB_SIZE_FR-2]\n",
        "    )\n",
        "    \n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_J2vsi5Mutr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "23dd12ae-70af-44cc-a917-ef8776107dca"
      },
      "source": [
        "translate(\"This is a really powerful tool!\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6a9614453ad5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This is a really powerful tool!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-01188c54fdb0>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     predicted_sentence = tokenizer_fr.decode(\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mVOCAB_SIZE_FR\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-ebdd16957e03>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(inp_sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0minp_sentence\u001b[0m \u001b[0;34m=\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mVOCAB_SIZE_EN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtokenizer_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_sentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mVOCAB_SIZE_EN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0menc_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVOCAB_SIZE_FR\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'VOCAB_SIZE_EN' is not defined"
          ]
        }
      ]
    }
  ]
}